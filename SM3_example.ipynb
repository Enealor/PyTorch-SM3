{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental validation of SM3\n",
    "\n",
    "Below we use three optimization algorithms to train a simple neural network classifer in order to experimentally compare the different algorithms. The learning rate for each algorithm was chosen by experimenting with fractions until the top-1 accuracy was at least 90%. (A more proper comparison would have been to leave the validation set out entirely until the end. However, the method is defended in the source; my concern was the correctness of my implementation.) After this, I chose a different seed.\n",
    "\n",
    "The highlight is that each achieves comparable success on this test problem despite drastic differences in the number of parameters that are tracked. The network's top-choice classification is correct at least 95% regardless of which optimizer is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Comparisons\n",
    "\n",
    "Below is the code used to compare the optimizers. A standardized weighted network is created. The weights are then updated using the chosen optimizer. When the updates are complete, the network is tested on the validation set. From the test, the negative log-likelihood and the top-1 accuracy is tracked. The number of parameters in the state dictionary of each optimizer is also counted.\n",
    "\n",
    "The negative log-likelihood is computed using `torch.nn.CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from SM3 import SM3\n",
    "\n",
    "batch_size = 100\n",
    "training_set = MNIST(root='D:\\Code\\data', train=True, download=False, transform=ToTensor())\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_set, batch_size=batch_size, shuffle=False)\n",
    "testing_set = MNIST(root='D:\\Code\\data', train=False, download=False, transform=ToTensor())\n",
    "testing_loader = torch.utils.data.DataLoader(dataset=testing_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "repeats = 10\n",
    "epochs = len(training_loader) * repeats\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network():\n",
    "    # Creates an example network using the same seed.\n",
    "    torch.manual_seed(64)\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28**2, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    )\n",
    "    return net\n",
    "\n",
    "def test_optimizer(optim_fn, params, lr_lambda=None):\n",
    "    net = make_network()\n",
    "    net.to(device)\n",
    "    opt = optim_fn(net.parameters(), **params)\n",
    "    if callable(lr_lambda):\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    train_net(epochs, net, opt, scheduler)\n",
    "    loss, correct = test_net(net)\n",
    "    count = count_param_size(opt)\n",
    "    return {'loss': loss, 'correct': correct, 'count': count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(epochs, net, opt, scheduler=None):\n",
    "    i = 0\n",
    "    for _ in range(repeats):\n",
    "        for batch_nb, batch in enumerate(training_loader):\n",
    "            opt.zero_grad()\n",
    "\n",
    "            images = batch[0].to(device)\n",
    "            targets = batch[1].to(device)\n",
    "\n",
    "            label = net(images)\n",
    "\n",
    "            # We are minimizing cross entropy loss\n",
    "            loss = loss_fn(label, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            i += 1\n",
    "            if i % (epochs // 1000) == 0:\n",
    "                print('\\r{0:.1%}'.format(i / epochs), end='')\n",
    "    print('') # clear line\n",
    "            \n",
    "def test_net(net):\n",
    "    testing_loss = 0.\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for images, targets in testing_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            label = net(images)\n",
    "\n",
    "            # Correct is tracked by seeing if the index of the max\n",
    "            # matches the label, and then adding these truth values.\n",
    "            correct += torch.sum(label.argmax(1) == targets)\n",
    "            testing_loss += loss_fn(label, targets)\n",
    "\n",
    "    return testing_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_param_size(opt):\n",
    "    count = 0\n",
    "    for state_value in opt.state.values():\n",
    "        for value in state_value.values():\n",
    "            if torch.is_tensor(value):\n",
    "                count += value.numel()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Adam\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Testing Adam')\n",
    "summary_Adam = test_optimizer(torch.optim.Adam, {'lr': 0.01})\n",
    "summary_Adam['name'] = 'Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Adagrad\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Testing Adagrad')\n",
    "summary_Adagrad = test_optimizer(torch.optim.Adagrad, {'lr': 0.1})\n",
    "summary_Adagrad['name'] = 'Adagrad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SM3-II\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Testing SM3-II')\n",
    "summary_SM3 = test_optimizer(SM3, {'lr': 0.1})\n",
    "summary_SM3['name'] = 'SM3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SM3-II with warm-up for first 5%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Testing SM3-II with warm-up for first 5%')\n",
    "lr_lambda = lambda epoch: min(1., (epoch / (0.05 * epochs)) ** 2)\n",
    "summary_SM3v2 = test_optimizer(SM3, {'lr': .5}, lr_lambda)\n",
    "summary_SM3v2['name'] = 'SM3 v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SM3-II with warm-up for first 5% and decay during last 10%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Testing SM3-II with warm-up for first 5% and decay during last 10%')\n",
    "lr_lambda = lambda epoch: min(1., (epoch / (0.05 * epochs)) ** 2, (epochs - epoch) / (0.1 * epochs))\n",
    "summary_SM3v3 = test_optimizer(SM3, {'lr': .5}, lr_lambda)\n",
    "summary_SM3v3['name'] = 'SM3 v3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Below we list the optimizers along with the cumulative negative log-likelihood and top-1 of the updated network, as well as the number of parameters used and the paramater count relative to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names  \tLoss\tCorrect\tParams\tRelative Params\n",
      "Adam   \t20.03\t9684.0\t1071636\t2.0\n",
      "Adagrad\t12.33\t9712.0\t535818\t1.0\n",
      "SM3    \t9.729\t9702.0\t3108\t0.0058\n",
      "SM3 v2 \t14.78\t9628.0\t3108\t0.0058\n",
      "SM3 v3 \t9.908\t9725.0\t3108\t0.0058\n"
     ]
    }
   ],
   "source": [
    "network_count = summary_Adagrad['count'] # Has the same number of parameters as the network\n",
    "print('Names  \\tLoss\\tCorrect\\tParams\\tRelative Params')\n",
    "for summary in [summary_Adam, summary_Adagrad, summary_SM3, summary_SM3v2, summary_SM3v3]:\n",
    "    print('{0:7}\\t{1:.4}\\t{2}\\t{3}\\t{4:.3%}'.format(\n",
    "        summary['name'],\n",
    "        summary['loss'],\n",
    "        summary['correct'],\n",
    "        summary['count'],\n",
    "        summary['count']/network_count)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}